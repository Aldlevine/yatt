{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Yatt\n",
    "\n",
    "Yatt is yet another PyTorch trainer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Size, Tensor, nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets as vdata\n",
    "from torchvision import transforms as vtransforms\n",
    "from torchvision import utils as vutils\n",
    "\n",
    "from yatt import DataLoaderConfig, OptimizerConfig, HParams, Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        child: nn.Module,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._child = child\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self._child.forward(x)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_shape: tuple[int, int, int],\n",
    "        hidden_dims: list[int],\n",
    "        latent_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        channels = [in_shape[0], *hidden_dims]\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(ch1, ch2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.SELU(),\n",
    "                    Residual(\n",
    "                        nn.Conv2d(ch2, ch2, kernel_size=3, stride=1, padding=1),\n",
    "                    ),\n",
    "                    nn.SELU(),\n",
    "                ) for ch1,ch2 in zip(channels[:-1], channels[1:])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        channels = list(reversed(channels))\n",
    "        self.decoder = nn.Sequential(\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(ch1, ch2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                    nn.SELU(),\n",
    "                    Residual(\n",
    "                        nn.Conv2d(ch2, ch2, kernel_size=3, stride=1, padding=1),\n",
    "                    ),\n",
    "                    nn.SELU(),\n",
    "                ) for ch1,ch2 in zip(channels[:-1], channels[1:])\n",
    "            ],\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        z = self.encoder(x)\n",
    "        xhat = self.decoder(z)\n",
    "        return xhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooParams(HParams):\n",
    "    dataset: Literal[\"cifar10\", \"celeba\", \"fgvc\"]\n",
    "    img_shape: tuple[int, int, int]\n",
    "    hidden_dims: list[int]\n",
    "    latent_dim: int\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_workers: int = (os.cpu_count() or 0) // 2\n",
    "\n",
    "\n",
    "class MyTrainer(Trainer[PooParams, AutoEncoder]):\n",
    "\n",
    "    @classmethod\n",
    "    def configure_model(cls, hp: PooParams) -> nn.Module:\n",
    "        model = AutoEncoder(\n",
    "            in_shape=hp.img_shape,\n",
    "            hidden_dims=hp.hidden_dims,\n",
    "            latent_dim=hp.latent_dim,\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def configure_optimizer(cls, hp: PooParams, model: AutoEncoder) -> OptimizerConfig:\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)\n",
    "        return OptimizerConfig(optimizer, lr_scheduler)\n",
    "    \n",
    "    @classmethod\n",
    "    def configure_grad_scaler(cls, hp: PooParams, model: AutoEncoder) -> None:\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    def configure_data_loaders(cls, hp: PooParams) -> DataLoaderConfig:\n",
    "        transform = vtransforms.Compose([\n",
    "            vtransforms.Resize(hp.img_shape[-2:]),\n",
    "            vtransforms.CenterCrop(hp.img_shape[-2:]),\n",
    "            vtransforms.ToTensor(),\n",
    "            vtransforms.Normalize(0.5, 0.5),\n",
    "        ])\n",
    "        match hp.dataset:\n",
    "            case \"cifar10\":\n",
    "                train_ds = vdata.CIFAR10(\"../data\", train=True, transform=transform, download=True)\n",
    "                train_ds, val_ds = data.random_split(train_ds, [0.9, 0.1])\n",
    "                test_ds = vdata.CIFAR10(\"../data\", train=False, transform=transform, download=True)\n",
    "            case \"celeba\":\n",
    "                train_ds = vdata.CelebA(\"../data\", split=\"train\", transform=transform, download=True)\n",
    "                val_ds = vdata.CelebA(\"../data\", split=\"valid\", transform=transform, download=True)\n",
    "                test_ds = vdata.CelebA(\"../data\", split=\"test\", transform=transform, download=True)\n",
    "            case \"fgvc\":\n",
    "                train_ds = vdata.FGVCAircraft(\"../data\", \"train\", transform=transform, download=True)\n",
    "                val_ds = vdata.FGVCAircraft(\"../data\", \"val\", transform=transform, download=True)\n",
    "                test_ds = vdata.FGVCAircraft(\"../data\", \"test\", transform=transform, download=True)\n",
    "            case _:\n",
    "                raise ValueError\n",
    "\n",
    "        train_dl = data.DataLoader(train_ds,\n",
    "                                   shuffle=True,\n",
    "                                   batch_size=hp.batch_size,\n",
    "                                   pin_memory=True,\n",
    "                                   num_workers=hp.num_workers,\n",
    "                                   persistent_workers=hp.num_workers > 0)\n",
    "        val_dl = data.DataLoader(val_ds,\n",
    "                                 batch_size=hp.batch_size,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=hp.num_workers,\n",
    "                                 persistent_workers=hp.num_workers > 0)\n",
    "        test_dl = data.DataLoader(test_ds,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=hp.num_workers,\n",
    "                                 persistent_workers=hp.num_workers > 0)\n",
    "        return DataLoaderConfig(\n",
    "            train=train_dl,\n",
    "            val=val_dl,\n",
    "            test=test_dl,\n",
    "        )\n",
    "\n",
    "    def get_loss(self, x: Tensor) -> Tensor:\n",
    "        xhat = self.model(x)\n",
    "        loss = torch.nn.functional.mse_loss(x, xhat)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def val_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def test_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "\n",
    "    def train_epoch_begin(self) -> None:\n",
    "        pass\n",
    "    def train_epoch_end(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def val_epoch_begin(self) -> None:\n",
    "        pass\n",
    "    def val_epoch_end(self) -> None:\n",
    "        if self.data_loaders.val == None:\n",
    "            return\n",
    "        x = next(iter(self.data_loaders.val))[0][:8].to(self.device)\n",
    "        y = self.model(x)\n",
    "        grid = vutils.make_grid(torch.cat([x, y]), normalize=True)\n",
    "        self.log_image(\"val/sample\", grid, self.epoch)\n",
    "        self.log_graph(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- fix widget style -->\n",
    "\n",
    "<style>\n",
    "    html .widget-html {\n",
    "        color: white !important;\n",
    "        mix-blend-mode: difference;\n",
    "    }\n",
    "\n",
    "    html .cell-output-ipywidget-background {\n",
    "        background: transparent !important;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = PooParams(\n",
    "    dataset=\"celeba\",\n",
    "    img_shape=(3,64,64),\n",
    "    hidden_dims=[16, 32, 64, 128],\n",
    "    latent_dim=512,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    f\"auto_encoder.{hp.dataset}.img_shape={hp.img_shape}.latent_dim={hp.latent_dim}\",\n",
    "    save_best_count=5,\n",
    "    max_epochs=1000,\n",
    "    log_interval=200,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "\n",
    "trainer.configure(hp)\n",
    "# trainer.configure_checkpoint(\"runs/auto_encoder.celeba.img_shape=(3, 64, 64).latent_dim=512/2023-03-14@14:33:31/checkpoints/latest.loss=0.018214423209428787.epoch=1.ckpt\")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca8d1fa01cdb653854c58e287ef85574a93ccbcde6df450724a3a644d34e1060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
