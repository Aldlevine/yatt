{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Yatt\n",
    "\n",
    "Yatt is yet another PyTorch trainer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Size, Tensor, nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets as vdata\n",
    "from torchvision import transforms as vtransforms\n",
    "from torchvision import utils as vutils\n",
    "\n",
    "from yatt import DataLoaderConfig, OptimizerConfig, HParams, Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_shape: tuple[int, int, int],\n",
    "        hidden_dims: list[int],\n",
    "        latent_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        ch = [in_shape[0], *hidden_dims]\n",
    "        hidden_size: np.ndarray = np.fromiter(in_shape[1:], object)\n",
    "        hidden_size //= 2**(len(hidden_dims) - 1)\n",
    "        hidden_shape = (hidden_dims[-1], *hidden_size)\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(ch[i],\n",
    "                              ch[i + 1],\n",
    "                              kernel_size=3,\n",
    "                              stride=1 + (i > 0),\n",
    "                              padding=1),\n",
    "                    nn.BatchNorm2d(ch[i + 1]),\n",
    "                    nn.SELU(),\n",
    "                    nn.Conv2d(ch[i + 1],\n",
    "                              ch[i + 1],\n",
    "                              kernel_size=3,\n",
    "                              stride=1,\n",
    "                              padding=1),\n",
    "                    nn.BatchNorm2d(ch[i + 1]),\n",
    "                    nn.SELU(),\n",
    "                ) for i in range(len(ch) - 1)\n",
    "            ],\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(int(np.prod(hidden_shape)), latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, int(np.prod(hidden_shape))),\n",
    "            nn.Unflatten(1, Size(hidden_shape)),\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "                    nn.Conv2d(ch[i],\n",
    "                              ch[i - 1],\n",
    "                              kernel_size=3,\n",
    "                              stride=1,\n",
    "                              padding=1),\n",
    "                    nn.BatchNorm2d(ch[i - 1]),\n",
    "                    nn.SELU(),\n",
    "                ) for i in range(len(ch) - 1, 0, -1)\n",
    "            ],\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.downsample = vtransforms.Resize(in_shape[-2:], interpolation=vtransforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        z = self.encoder(x)\n",
    "        xhat_large = self.decoder(z)\n",
    "        xhat = self.downsample(xhat_large.detach())\n",
    "        return xhat, xhat_large"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HP(HParams):\n",
    "    dataset: Literal[\"cifar10\", \"celeba\", \"fgvc\"]\n",
    "    img_shape: tuple[int, int, int]\n",
    "    hidden_dims: list[int]\n",
    "    latent_dim: int\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_workers: int = (os.cpu_count() or 0) // 2\n",
    "\n",
    "\n",
    "class MyTrainer(Trainer[HP]):\n",
    "\n",
    "    def configure_model(self) -> nn.Module:\n",
    "        model = AutoEncoder(\n",
    "            in_shape=self.hparams.img_shape,\n",
    "            hidden_dims=self.hparams.hidden_dims,\n",
    "            latent_dim=self.hparams.latent_dim,\n",
    "        )\n",
    "\n",
    "        self.downscale = vtransforms.Resize(self.hparams.img_shape[-2:])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizer(self) -> OptimizerConfig:\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)\n",
    "        return OptimizerConfig(optimizer, lr_scheduler)\n",
    "\n",
    "    def configure_data_loaders(self) -> DataLoaderConfig:\n",
    "        transform = vtransforms.Compose([\n",
    "            vtransforms.Resize([s * 2 for s in self.hparams.img_shape[-2:]]),\n",
    "            vtransforms.CenterCrop([s * 2 for s in self.hparams.img_shape[-2:]]),\n",
    "            vtransforms.ToTensor(),\n",
    "            vtransforms.Normalize(0.5, 0.5),\n",
    "        ])\n",
    "        match self.hparams.dataset:\n",
    "            case \"cifar10\":\n",
    "                train_ds = vdata.CIFAR10(\"../data\", train=True, transform=transform, download=True)\n",
    "                train_ds, val_ds = data.random_split(train_ds, [0.9, 0.1])\n",
    "                test_ds = vdata.CIFAR10(\"../data\", train=False, transform=transform, download=True)\n",
    "            case \"celeba\":\n",
    "                train_ds = vdata.CelebA(\"../data\", split=\"train\", transform=transform, download=True)\n",
    "                val_ds = vdata.CelebA(\"../data\", split=\"valid\", transform=transform, download=True)\n",
    "                test_ds = vdata.CelebA(\"../data\", split=\"test\", transform=transform, download=True)\n",
    "            case \"fgvc\":\n",
    "                train_ds = vdata.FGVCAircraft(\"../data\", \"train\", transform=transform, download=True)\n",
    "                val_ds = vdata.FGVCAircraft(\"../data\", \"val\", transform=transform, download=True)\n",
    "                test_ds = vdata.FGVCAircraft(\"../data\", \"test\", transform=transform, download=True)\n",
    "            case _:\n",
    "                raise ValueError\n",
    "\n",
    "        train_dl = data.DataLoader(train_ds,\n",
    "                                   shuffle=True,\n",
    "                                   batch_size=self.hparams.batch_size,\n",
    "                                   pin_memory=True,\n",
    "                                   num_workers=self.hparams.num_workers,\n",
    "                                   persistent_workers=self.hparams.num_workers > 0)\n",
    "        val_dl = data.DataLoader(val_ds,\n",
    "                                 batch_size=self.hparams.batch_size,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=self.hparams.num_workers,\n",
    "                                 persistent_workers=self.hparams.num_workers > 0)\n",
    "        test_dl = data.DataLoader(test_ds,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=self.hparams.num_workers,\n",
    "                                 persistent_workers=self.hparams.num_workers > 0)\n",
    "        return DataLoaderConfig(\n",
    "            train=train_dl,\n",
    "            val=val_dl,\n",
    "            test=test_dl,\n",
    "        )\n",
    "\n",
    "    def get_loss(self, x: Tensor) -> Tensor:\n",
    "        x_small = self.downscale(x)\n",
    "        _, y_large = self.model(x_small)\n",
    "        loss_large = torch.nn.functional.mse_loss(x, y_large)\n",
    "        return loss_large\n",
    "\n",
    "    def train_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def val_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def test_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "\n",
    "    def train_epoch_begin(self) -> None:\n",
    "        pass\n",
    "    def train_epoch_end(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def val_epoch_begin(self) -> None:\n",
    "        pass\n",
    "    def val_epoch_end(self) -> None:\n",
    "        if self.data_loaders.val == None:\n",
    "            return\n",
    "        x = next(iter(self.data_loaders.val))[0][:8].to(self.device)\n",
    "        x = self.downscale(x)\n",
    "        y, y_large = self.model(x)\n",
    "        grid = vutils.make_grid(torch.cat([x, y]), normalize=True)\n",
    "        self.log_image(\"val/sample\", grid, self.epoch)\n",
    "        self.log_graph(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- fix widget style -->\n",
    "\n",
    "<style>\n",
    "    html .widget-html {\n",
    "        color: white !important;\n",
    "        mix-blend-mode: difference;\n",
    "    }\n",
    "\n",
    "    html .cell-output-ipywidget-background {\n",
    "        background: transparent !important;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HP(\n",
    "    dataset=\"celeba\",\n",
    "    img_shape=(3,64,64),\n",
    "    hidden_dims=[16, 32, 64, 128],\n",
    "    latent_dim=512,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    f\"auto_encoder.{hp.dataset}.img_shape={hp.img_shape}.latent_dim={hp.latent_dim}\",\n",
    "    save_best_count=5,\n",
    "    max_epochs=1000,\n",
    "    log_interval=200,\n",
    "    device=torch.device(\"cuda\"),\n",
    ")\n",
    "\n",
    "# trainer.configure(hp)\n",
    "\n",
    "# trainer.configure_checkpoint(\"runs/auto_encoder.img_shape=(3, 32, 32).latent_dim=512/2023-02-16@08:57:48/checkpoints/latest.loss=0.00773597089573741.epoch=334.ckpt\")\n",
    "trainer.configure_checkpoint(\"runs/auto_encoder.celeba.img_shape=(3, 64, 64).latent_dim=512/2023-02-16@20:56:11/checkpoints/latest.loss=0.012108607217669487.epoch=56.ckpt\")\n",
    "\n",
    "trainer.train()\n",
    "# print(trainer.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca8d1fa01cdb653854c58e287ef85574a93ccbcde6df450724a3a644d34e1060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
