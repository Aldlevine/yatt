{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Yatt\n",
    "\n",
    "Yatt is yet another PyTorch trainer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Size, Tensor, nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets as vdata\n",
    "from torchvision import transforms as vtransforms\n",
    "from torchvision import utils as vutils\n",
    "\n",
    "from yatt import DataLoaderConfig, OptimizerConfig, HParams, Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        child: nn.Module,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._child = child\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self._child.forward(x)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_shape: tuple[int, int, int],\n",
    "        hidden_dims: list[int],\n",
    "        latent_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        channels = [in_shape[0], *hidden_dims]\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(ch1, ch2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.SELU(),\n",
    "                    Residual(\n",
    "                        nn.Conv2d(ch2, ch2, kernel_size=3, stride=1, padding=1),\n",
    "                    ),\n",
    "                    nn.SELU(),\n",
    "                ) for ch1,ch2 in zip(channels[:-1], channels[1:])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        channels = list(reversed(channels))\n",
    "        self.decoder = nn.Sequential(\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(ch1, ch2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                    nn.SELU(),\n",
    "                    Residual(\n",
    "                        nn.Conv2d(ch2, ch2, kernel_size=3, stride=1, padding=1),\n",
    "                    ),\n",
    "                    nn.SELU(),\n",
    "                ) for ch1,ch2 in zip(channels[:-1], channels[1:])\n",
    "            ],\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        z = self.encoder(x)\n",
    "        xhat = self.decoder(z)\n",
    "        return xhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooParams(HParams):\n",
    "    dataset: Literal[\"cifar10\", \"celeba\", \"fgvc\"]\n",
    "    img_shape: tuple[int, int, int]\n",
    "    hidden_dims: list[int]\n",
    "    latent_dim: int\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_workers: int = (os.cpu_count() or 0) // 2\n",
    "\n",
    "\n",
    "class MyTrainer(Trainer[PooParams, AutoEncoder]):\n",
    "\n",
    "    @classmethod\n",
    "    def configure_model(cls, hp: PooParams) -> nn.Module:\n",
    "        model = AutoEncoder(\n",
    "            in_shape=hp.img_shape,\n",
    "            hidden_dims=hp.hidden_dims,\n",
    "            latent_dim=hp.latent_dim,\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def configure_optimizer(cls, hp: PooParams, model: AutoEncoder) -> OptimizerConfig:\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)\n",
    "        return OptimizerConfig(optimizer, lr_scheduler)\n",
    "\n",
    "    @classmethod\n",
    "    def configure_data_loaders(cls, hp: PooParams) -> DataLoaderConfig:\n",
    "        transform = vtransforms.Compose([\n",
    "            vtransforms.Resize(hp.img_shape[-2:]),\n",
    "            vtransforms.CenterCrop(hp.img_shape[-2:]),\n",
    "            vtransforms.ToTensor(),\n",
    "            vtransforms.Normalize(0.5, 0.5),\n",
    "        ])\n",
    "        match hp.dataset:\n",
    "            case \"cifar10\":\n",
    "                train_ds = vdata.CIFAR10(\"../data\", train=True, transform=transform, download=True)\n",
    "                train_ds, val_ds = data.random_split(train_ds, [0.9, 0.1])\n",
    "                test_ds = vdata.CIFAR10(\"../data\", train=False, transform=transform, download=True)\n",
    "            case \"celeba\":\n",
    "                train_ds = vdata.CelebA(\"../data\", split=\"train\", transform=transform, download=True)\n",
    "                val_ds = vdata.CelebA(\"../data\", split=\"valid\", transform=transform, download=True)\n",
    "                test_ds = vdata.CelebA(\"../data\", split=\"test\", transform=transform, download=True)\n",
    "            case \"fgvc\":\n",
    "                train_ds = vdata.FGVCAircraft(\"../data\", \"train\", transform=transform, download=True)\n",
    "                val_ds = vdata.FGVCAircraft(\"../data\", \"val\", transform=transform, download=True)\n",
    "                test_ds = vdata.FGVCAircraft(\"../data\", \"test\", transform=transform, download=True)\n",
    "            case _:\n",
    "                raise ValueError\n",
    "\n",
    "        train_dl = data.DataLoader(train_ds,\n",
    "                                   shuffle=True,\n",
    "                                   batch_size=hp.batch_size,\n",
    "                                   pin_memory=True,\n",
    "                                   num_workers=hp.num_workers,\n",
    "                                   persistent_workers=hp.num_workers > 0)\n",
    "        val_dl = data.DataLoader(val_ds,\n",
    "                                 batch_size=hp.batch_size,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=hp.num_workers,\n",
    "                                 persistent_workers=hp.num_workers > 0)\n",
    "        test_dl = data.DataLoader(test_ds,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=hp.num_workers,\n",
    "                                 persistent_workers=hp.num_workers > 0)\n",
    "        return DataLoaderConfig(\n",
    "            train=train_dl,\n",
    "            val=val_dl,\n",
    "            test=test_dl,\n",
    "        )\n",
    "\n",
    "    def get_loss(self, x: Tensor) -> Tensor:\n",
    "        xhat = self.model(x)\n",
    "        loss = torch.nn.functional.mse_loss(x, xhat)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def val_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def test_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "\n",
    "    def train_epoch_begin(self) -> None:\n",
    "        pass\n",
    "    def train_epoch_end(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def val_epoch_begin(self) -> None:\n",
    "        pass\n",
    "    def val_epoch_end(self) -> None:\n",
    "        if self.data_loaders.val == None:\n",
    "            return\n",
    "        x = next(iter(self.data_loaders.val))[0][:8].to(self.device)\n",
    "        y = self.model(x)\n",
    "        grid = vutils.make_grid(torch.cat([x, y]), normalize=True)\n",
    "        self.log_image(\"val/sample\", grid, self.epoch)\n",
    "        self.log_graph(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- fix widget style -->\n",
       "\n",
       "<style>\n",
       "    html .widget-html {\n",
       "        color: white !important;\n",
       "        mix-blend-mode: difference;\n",
       "    }\n",
       "\n",
       "    html .cell-output-ipywidget-background {\n",
       "        background: transparent !important;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- fix widget style -->\n",
    "\n",
    "<style>\n",
    "    html .widget-html {\n",
    "        color: white !important;\n",
    "        mix-blend-mode: difference;\n",
    "    }\n",
    "\n",
    "    html .cell-output-ipywidget-background {\n",
    "        background: transparent !important;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────────────────────┐\n",
      "│             AutoEncoder             │\n",
      "└─────────────────────────────────────┘\n",
      "┌─────────────────────────────────────┐\n",
      "│               HParams               │\n",
      "├─────────────────┬───────────────────┤\n",
      "│ dataset         │ celeba            │\n",
      "│ img_shape       │ (3, 64, 64)       │\n",
      "│ hidden_dims     │ [16, 32, 64, 128] │\n",
      "│ latent_dim      │ 512               │\n",
      "│ learning_rate   │ 0.001             │\n",
      "│ batch_size      │ 512               │\n",
      "│ num_workers     │ 8                 │\n",
      "└─────────────────┴───────────────────┘\n",
      "┌─────────────────────────────────────┐\n",
      "│                Stats                │\n",
      "├─────────────────┬───────────────────┤\n",
      "│ Parameter Count │ 32                │\n",
      "│ Parameter Size  │ 1.7MiB            │\n",
      "│ Buffer Count    │ 0                 │\n",
      "│ Buffer Size     │ 0.0B              │\n",
      "│ Total Size      │ 1.7MiB            │\n",
      "└─────────────────┴───────────────────┘\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355c460195b744c7b33010a4d70669e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 2:   0%|          | 0/318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# trainer.configure(hp)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m trainer\u001b[39m.\u001b[39mconfigure_checkpoint(\u001b[39m\"\u001b[39m\u001b[39mruns/auto_encoder.celeba.img_shape=(3, 64, 64).latent_dim=512/2023-03-14@14:33:31/checkpoints/latest.loss=0.018214423209428787.epoch=1.ckpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Documents/Projects/yatt/yatt/trainer.py:230\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_loaders\u001b[39m.\u001b[39mtrain \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop()\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_loaders\u001b[39m.\u001b[39mval \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_val_loop()\n",
      "File \u001b[0;32m~/Documents/Projects/yatt/yatt/trainer.py:480\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[1;32m    479\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_epoch_begin()\n\u001b[0;32m--> 480\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loop(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, get_loss, log_epoch_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_epoch_end()\n",
      "File \u001b[0;32m~/Documents/Projects/yatt/yatt/trainer.py:451\u001b[0m, in \u001b[0;36mTrainer._loop\u001b[0;34m(self, stage, get_loss, log_epoch_only)\u001b[0m\n\u001b[1;32m    449\u001b[0m batch \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m batch]\n\u001b[1;32m    450\u001b[0m loss \u001b[39m=\u001b[39m get_loss(batch, batch_idx)\n\u001b[0;32m--> 451\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m    452\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    453\u001b[0m total_loss_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hp = PooParams(\n",
    "    dataset=\"celeba\",\n",
    "    img_shape=(3,64,64),\n",
    "    hidden_dims=[16, 32, 64, 128],\n",
    "    latent_dim=512,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    f\"auto_encoder.{hp.dataset}.img_shape={hp.img_shape}.latent_dim={hp.latent_dim}\",\n",
    "    save_best_count=5,\n",
    "    max_epochs=1000,\n",
    "    log_interval=200,\n",
    "    device=torch.device(\"cuda\"),\n",
    ")\n",
    "\n",
    "# trainer.configure(hp)\n",
    "trainer.configure_checkpoint(\"runs/auto_encoder.celeba.img_shape=(3, 64, 64).latent_dim=512/2023-03-14@14:33:31/checkpoints/latest.loss=0.018214423209428787.epoch=1.ckpt\")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca8d1fa01cdb653854c58e287ef85574a93ccbcde6df450724a3a644d34e1060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
