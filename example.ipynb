{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Yatt\n",
    "\n",
    "Yatt is yet another PyTorch trainer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Size, Tensor, nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets as vdata\n",
    "from torchvision import transforms as vtransforms\n",
    "from torchvision import utils as vutils\n",
    "\n",
    "from yatt import DataLoaderConfig, HParams, Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_shape: tuple[int, int, int],\n",
    "        hidden_dims: list[int],\n",
    "        latent_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        ch = [in_shape[0], *hidden_dims]\n",
    "        hidden_size: np.ndarray = np.fromiter(in_shape[1:], object)\n",
    "        hidden_size //= 2**(len(hidden_dims) - 1)\n",
    "        hidden_shape = (hidden_dims[-1], *hidden_size)\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(ch[i],\n",
    "                              ch[i + 1],\n",
    "                              kernel_size=3,\n",
    "                              stride=1 + (i > 0),\n",
    "                              padding=1),\n",
    "                    nn.BatchNorm2d(ch[i + 1]),\n",
    "                    nn.SELU(),\n",
    "                    nn.Conv2d(ch[i + 1],\n",
    "                              ch[i + 1],\n",
    "                              kernel_size=3,\n",
    "                              stride=1,\n",
    "                              padding=1),\n",
    "                    nn.BatchNorm2d(ch[i + 1]),\n",
    "                    nn.SELU(),\n",
    "                ) for i in range(len(ch) - 1)\n",
    "            ],\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(int(np.prod(hidden_shape)), latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, int(np.prod(hidden_shape))),\n",
    "            nn.Unflatten(1, Size(hidden_shape)),\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "                    nn.Conv2d(ch[i],\n",
    "                              ch[i - 1],\n",
    "                              kernel_size=3,\n",
    "                              stride=1,\n",
    "                              padding=1),\n",
    "                    nn.BatchNorm2d(ch[i - 1]),\n",
    "                    nn.SELU(),\n",
    "                ) for i in range(len(ch) - 1, 0, -1)\n",
    "            ],\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.downsample = vtransforms.Resize(in_shape[-2:], interpolation=vtransforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        z = self.encoder(x)\n",
    "        xhat_large = self.decoder(z)\n",
    "        xhat = self.downsample(xhat_large.detach())\n",
    "        return xhat, xhat_large"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HP(HParams):\n",
    "    dataset: Literal[\"cifar10\", \"celeba\", \"fgvc\"]\n",
    "    img_shape: tuple[int, int, int]\n",
    "    hidden_dims: list[int]\n",
    "    latent_dim: int\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_workers: int = (os.cpu_count() or 0) // 2\n",
    "\n",
    "\n",
    "class MyTrainer(Trainer[HP]):\n",
    "\n",
    "    def configure_model(self) -> nn.Module:\n",
    "        model = AutoEncoder(\n",
    "            in_shape=self.hparams.img_shape,\n",
    "            hidden_dims=self.hparams.hidden_dims,\n",
    "            latent_dim=self.hparams.latent_dim,\n",
    "        )\n",
    "\n",
    "        self.downscale = vtransforms.Resize(self.hparams.img_shape[-2:])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizer(self) -> torch.optim.Optimizer:\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        return optimizer\n",
    "\n",
    "    def configure_data_loaders(self) -> DataLoaderConfig:\n",
    "        transform = vtransforms.Compose([\n",
    "            vtransforms.Resize([s * 2 for s in self.hparams.img_shape[-2:]]),\n",
    "            vtransforms.CenterCrop([s * 2 for s in self.hparams.img_shape[-2:]]),\n",
    "            vtransforms.ToTensor(),\n",
    "            vtransforms.Normalize(0.5, 0.5),\n",
    "        ])\n",
    "        match self.hparams.dataset:\n",
    "            case \"cifar10\":\n",
    "                train_ds = vdata.CIFAR10(\"../data\", train=True, transform=transform, download=True)\n",
    "                train_ds, val_ds = data.random_split(train_ds, [0.9, 0.1])\n",
    "                test_ds = vdata.CIFAR10(\"../data\", train=False, transform=transform, download=True)\n",
    "            case \"celeba\":\n",
    "                train_ds = vdata.CelebA(\"../data\", split=\"train\", transform=transform, download=True)\n",
    "                val_ds = vdata.CelebA(\"../data\", split=\"valid\", transform=transform, download=True)\n",
    "                test_ds = vdata.CelebA(\"../data\", split=\"test\", transform=transform, download=True)\n",
    "            case \"fgvc\":\n",
    "                train_ds = vdata.FGVCAircraft(\"../data\", \"train\", transform=transform, download=True)\n",
    "                val_ds = vdata.FGVCAircraft(\"../data\", \"val\", transform=transform, download=True)\n",
    "                test_ds = vdata.FGVCAircraft(\"../data\", \"test\", transform=transform, download=True)\n",
    "            case _:\n",
    "                raise ValueError\n",
    "\n",
    "        train_dl = data.DataLoader(train_ds,\n",
    "                                   shuffle=True,\n",
    "                                   batch_size=self.hparams.batch_size,\n",
    "                                   pin_memory=True,\n",
    "                                   num_workers=self.hparams.num_workers,\n",
    "                                   persistent_workers=self.hparams.num_workers > 0)\n",
    "        val_dl = data.DataLoader(val_ds,\n",
    "                                 batch_size=self.hparams.batch_size,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=self.hparams.num_workers,\n",
    "                                 persistent_workers=self.hparams.num_workers > 0)\n",
    "        test_dl = data.DataLoader(test_ds,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=self.hparams.num_workers,\n",
    "                                 persistent_workers=self.hparams.num_workers > 0)\n",
    "        return DataLoaderConfig(\n",
    "            train=train_dl,\n",
    "            val=val_dl,\n",
    "            test=test_dl,\n",
    "        )\n",
    "\n",
    "    def get_loss(self, x: Tensor) -> Tensor:\n",
    "        x_small = self.downscale(x)\n",
    "        _, y_large = self.model(x_small)\n",
    "        loss_large = torch.nn.functional.mse_loss(x, y_large)\n",
    "        return loss_large\n",
    "\n",
    "    def train_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def val_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def test_step(self, batch: list[Tensor], batch_idx: int) -> Tensor:\n",
    "        return self.get_loss(batch[0])\n",
    "\n",
    "    def train_epoch_end(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def val_epoch_end(self) -> None:\n",
    "        if self.data_loaders.val == None:\n",
    "            return\n",
    "        x = next(iter(self.data_loaders.val))[0][:8].to(self.device)\n",
    "        x = self.downscale(x)\n",
    "        y, y_large = self.model(x)\n",
    "        grid = vutils.make_grid(torch.cat([x, y]), normalize=True)\n",
    "        self.log_image(\"val/sample\", grid, self.epoch)\n",
    "        self.log_graph(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- fix widget style -->\n",
       "\n",
       "<style>\n",
       "    html .widget-html {\n",
       "        color: white !important;\n",
       "        mix-blend-mode: difference;\n",
       "    }\n",
       "\n",
       "    html .cell-output-ipywidget-background {\n",
       "        background: transparent !important;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- fix widget style -->\n",
    "\n",
    "<style>\n",
    "    html .widget-html {\n",
    "        color: white !important;\n",
    "        mix-blend-mode: difference;\n",
    "    }\n",
    "\n",
    "    html .cell-output-ipywidget-background {\n",
    "        background: transparent !important;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────────────────┐\n",
      "│              AutoEncoder               │\n",
      "└────────────────────────────────────────┘\n",
      "┌────────────────────────────────────────┐\n",
      "│                HParams                 │\n",
      "├─────────────────┬──────────────────────┤\n",
      "│ dataset         │ celeba               │\n",
      "│ img_shape       │ (3, 64, 64)          │\n",
      "│ hidden_dims     │ [16, 32, 64, 128]    │\n",
      "│ latent_dim      │ 512                  │\n",
      "│ learning_rate   │ 0.001                │\n",
      "│ batch_size      │ 512                  │\n",
      "│ num_workers     │ 8                    │\n",
      "└─────────────────┴──────────────────────┘\n",
      "┌────────────────────────────────────────┐\n",
      "│                 Stats                  │\n",
      "├─────────────────┬──────────────────────┤\n",
      "│ Parameter Count │ 52                   │\n",
      "│ Parameter Size  │ 33.5MiB              │\n",
      "│ Buffer Count    │ 36                   │\n",
      "│ Buffer Size     │ 4.7KiB               │\n",
      "│ Total Size      │ 33.5MiB              │\n",
      "└─────────────────┴──────────────────────┘\n",
      "┌────────────────────────────────────────┐\n",
      "│            Best Checkpoints            │\n",
      "│                                        │\n",
      "│      Epoch      │         Loss         │\n",
      "├─────────────────┼──────────────────────┤\n",
      "│ 57              │ 0.012176396325230598 │\n",
      "│ 58              │ 0.012288099154829979 │\n",
      "└─────────────────┴──────────────────────┘\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38612f74f0ea439f968e3111a0d95882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 59:   0%|          | 0/318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# trainer.configure(hp)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[39m# trainer.configure_checkpoint(\"runs/auto_encoder.img_shape=(3, 32, 32).latent_dim=512/2023-02-16@08:57:48/checkpoints/latest.loss=0.00773597089573741.epoch=334.ckpt\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m trainer\u001b[39m.\u001b[39mconfigure_checkpoint(\u001b[39m\"\u001b[39m\u001b[39mruns/auto_encoder.celeba.img_shape=(3, 64, 64).latent_dim=512/2023-02-16@20:56:11/checkpoints/latest.loss=0.012108607217669487.epoch=56.ckpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     24\u001b[0m \u001b[39m# print(trainer.model)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/yatt/yatt/trainer.py:189\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_loaders\u001b[39m.\u001b[39mtrain \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop()\n\u001b[1;32m    190\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_epoch_end()\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_loaders\u001b[39m.\u001b[39mval \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projects/yatt/yatt/trainer.py:440\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[0;32m--> 440\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loop(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, get_loss, log_epoch_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/Projects/yatt/yatt/trainer.py:413\u001b[0m, in \u001b[0;36mTrainer._loop\u001b[0;34m(self, stage, get_loss, log_epoch_only)\u001b[0m\n\u001b[1;32m    411\u001b[0m batch \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m batch]\n\u001b[1;32m    412\u001b[0m loss \u001b[39m=\u001b[39m get_loss(batch, batch_idx)\n\u001b[0;32m--> 413\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m    414\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    415\u001b[0m total_loss_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hp = HP(\n",
    "    dataset=\"celeba\",\n",
    "    img_shape=(3,64,64),\n",
    "    hidden_dims=[16, 32, 64, 128],\n",
    "    latent_dim=512,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    f\"auto_encoder.{hp.dataset}.img_shape={hp.img_shape}.latent_dim={hp.latent_dim}\",\n",
    "    save_best_count=5,\n",
    "    max_epochs=1000,\n",
    "    log_interval=200,\n",
    "    device=torch.device(\"cuda\"),\n",
    ")\n",
    "\n",
    "# trainer.configure(hp)\n",
    "\n",
    "# trainer.configure_checkpoint(\"runs/auto_encoder.img_shape=(3, 32, 32).latent_dim=512/2023-02-16@08:57:48/checkpoints/latest.loss=0.00773597089573741.epoch=334.ckpt\")\n",
    "trainer.configure_checkpoint(\"runs/auto_encoder.celeba.img_shape=(3, 64, 64).latent_dim=512/2023-02-16@20:56:11/checkpoints/latest.loss=0.012108607217669487.epoch=56.ckpt\")\n",
    "\n",
    "trainer.train()\n",
    "# print(trainer.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca8d1fa01cdb653854c58e287ef85574a93ccbcde6df450724a3a644d34e1060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
